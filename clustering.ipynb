{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 23:45:55.704731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 23:45:56.239001: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-11 23:45:57.905461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-11 23:45:57.905797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-11 23:45:57.905816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.cluster.vq import kmeans, whiten\n",
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for inference\n",
    "data_index = [\"H-15\"]\n",
    "\n",
    "# train data for clustering algorithm\n",
    "train_index = [\"A-7\", \"A-15\", \"B-11\", \"H-15\", \"F-9\", \"G-3\"]\n",
    "path_cores = \"TMA_cores_M06_M07_panels/M06/Cores/\"\n",
    "path_mxIF = \"Texts_small_coregistered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference data\n",
    "data_cores = [cv.imread(path_cores + index + \".png\") for index in data_index]\n",
    "data_mxIF = [pd.read_csv(path_mxIF + index + \".csv\") for index in data_index]\n",
    "\n",
    "# train data for clustering algorithm\n",
    "train_cores = [cv.imread(path_cores + index + \".png\") for index in train_index]\n",
    "train_mxIF = [pd.read_csv(path_mxIF + index + \".csv\") for index in train_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER = 50\n",
    "BATCH = 32\n",
    "CELL_SIZE = (32, 32)\n",
    "MXIF_FEATURES = [\"Nucleus PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MAX = np.zeros(len(MXIF_FEATURES))\n",
    "TOTAL_MIN = np.zeros(len(MXIF_FEATURES))\n",
    "\n",
    "for i, feature in enumerate(MXIF_FEATURES):\n",
    "    for core in train_mxIF:\n",
    "        current_max = core.loc[:,feature].max()\n",
    "        current_min = core.loc[:,feature].min()\n",
    "        if current_max > TOTAL_MAX[i]:\n",
    "            TOTAL_MAX[i] = current_max\n",
    "        if current_min < TOTAL_MIN[i]:\n",
    "            TOTAL_MIN[i] = current_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(mxIF, cores):\n",
    "    def data_generator():\n",
    "        for i in range(len(mxIF)):\n",
    "            X = mxIF[i].loc[:,'Cell X Position']\n",
    "            Y = mxIF[i].loc[:,'Cell Y Position']\n",
    "\n",
    "            for j,(x,y) in enumerate(zip(X, Y)):\n",
    "                x = float(x)\n",
    "                y = float(y)\n",
    "                if np.isnan(x) or np.isnan(y):\n",
    "                    continue\n",
    "                if round(x - CELL_SIZE[0]) < 0 or round(x + CELL_SIZE[0]) >= cores[i].shape[1]:\n",
    "                    continue\n",
    "                if round(y - CELL_SIZE[1]) < 0 or round(y + CELL_SIZE[1]) >= cores[i].shape[0]:\n",
    "                    continue\n",
    "\n",
    "                cell_image = cores[i][round(y-CELL_SIZE[1]):round(y+CELL_SIZE[1]),\n",
    "                                            round(x-CELL_SIZE[0]):round(x+CELL_SIZE[0])] / 255\n",
    "                    \n",
    "                cell_features = np.array(mxIF[i].loc[j, MXIF_FEATURES], dtype=np.float32)\n",
    "                cell_features = (cell_features - TOTAL_MIN) / TOTAL_MAX\n",
    "                    \n",
    "                if np.sum(np.isnan(cell_features)) != 0:\n",
    "                    continue\n",
    "\n",
    "                yield (cell_image, cell_features)\n",
    "                \n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_coordinate_generator():\n",
    "    for i in range(len(data_index)):\n",
    "        X = data_mxIF[i].loc[:,'Cell X Position']\n",
    "        Y = data_mxIF[i].loc[:,'Cell Y Position']\n",
    "\n",
    "        for j,(x,y) in enumerate(zip(X, Y)):\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            if np.isnan(x) or np.isnan(y):\n",
    "                continue\n",
    "            if round(x - CELL_SIZE[0]) < 0 or round(x + CELL_SIZE[0]) >= data_cores[i].shape[1]:\n",
    "                continue\n",
    "            if round(y - CELL_SIZE[1]) < 0 or round(y + CELL_SIZE[1]) >= data_cores[i].shape[0]:\n",
    "                continue\n",
    "\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 23:46:19.951467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-11 23:46:19.951559: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-11 23:46:19.951604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (icme-gpu1): /proc/driver/nvidia/version does not exist\n",
      "2023-03-11 23:46:19.953934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_generator(get_generator(train_mxIF, train_cores),\n",
    "                                       output_signature=(tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),\n",
    "                                                          tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32)))\n",
    "train_data = train_data.batch(BATCH)\n",
    "data = tf.data.Dataset.from_generator(get_generator(data_mxIF, data_cores),\n",
    "                                       output_signature=(tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),\n",
    "                                                          tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32)))\n",
    "data = data.batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = \"logs/autoencoder/baseline_big/20230309-181840/\"\n",
    "loaded_model = tf.saved_model.load(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# calculate the latent vectors of all the training data\n",
    "train_data_latent = []\n",
    "for i, elem in enumerate(train_data):\n",
    "    latent_he = loaded_model.encoder_conv(elem[0])\n",
    "    latent_mxIF = loaded_model.encoder_fnn(elem[1])\n",
    "    latent_he = latent_he.numpy()\n",
    "    latent_mxIF = latent_mxIF.numpy()\n",
    "    train_data_latent.append(np.concatenate([latent_he, latent_mxIF], axis=1))\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/scipy/cluster/vq.py:136: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  warnings.warn(\"Some columns have standard deviation zero. \"\n"
     ]
    }
   ],
   "source": [
    "# calculate K cluster centroids in the latent space\n",
    "K = 10\n",
    "train_vectors_latent = np.concatenate(train_data_latent, axis=0)\n",
    "train_vectors_latent = whiten(train_vectors_latent)\n",
    "centroids, distortion = kmeans(train_vectors_latent, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# calculate the latent representation of the data for inference\n",
    "data_latent = []\n",
    "for i, elem in enumerate(data):\n",
    "    latent_he = loaded_model.encoder_conv(elem[0])\n",
    "    latent_mxIF = loaded_model.encoder_fnn(elem[1])\n",
    "    latent_he = latent_he.numpy()\n",
    "    latent_mxIF = latent_mxIF.numpy()\n",
    "    data_latent.append(np.concatenate([latent_he, latent_mxIF], axis=1))\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cluster for each element in the inference data by finding closest centroid\n",
    "clusters = []\n",
    "for batch in data_latent:\n",
    "    for i in range(batch.shape[0]):\n",
    "        vector = batch[i,:]\n",
    "        vector = vector[:,np.newaxis]\n",
    "        dist = np.linalg.norm(np.repeat(vector, K, axis=1) - centroids.T, axis=0)\n",
    "        clusters.append(np.argmin(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colors for the K clusters\n",
    "colormap = ['#0000FF', '#8A2BE2', '#FF4040', '#8A360F', '#98F5FF', '#FF6103', '#7FFF00', '#EEE8CD', '#FFB90F', '#556B2F', '#EE1289']\n",
    "colormap = [ImageColor.getcolor(color, \"RGB\") for color in colormap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the clustered cells in thre original H&E image for visualization\n",
    "src = data_cores[0]\n",
    "cells = cell_coordinate_generator()\n",
    "for i, (x, y) in enumerate(cells):\n",
    "    if i >= len(clusters):\n",
    "        break\n",
    "    cv.circle(src, (int(x),int(y)), radius=5, color=colormap[int(clusters[i])], thickness=-1)\n",
    "\n",
    "cv.imwrite(\"cluster.png\", src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
