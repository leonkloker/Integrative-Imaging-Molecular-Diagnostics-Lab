{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 21:08:43.948933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 21:08:44.244850: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-08 21:08:45.312015: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-08 21:08:45.312190: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-08 21:08:45.312204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.cluster.vq import kmeans, whiten\n",
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = [\"A-7\", \"A-15\", \"B-11\", \"H-15\", \"F-9\", \"G-3\"]\n",
    "path_cores = \"TMA_cores_M06_M07_panels/M06/Cores/\"\n",
    "path_mxIF = \"Texts_small_coregistered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cores = [cv.imread(path_cores + index + \".png\") for index in train_index]\n",
    "train_mxIF = [pd.read_csv(path_mxIF + index + \".csv\") for index in train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER = 50\n",
    "BATCH = 32\n",
    "VAL_SPLIT = 0.04\n",
    "CELL_SIZE = (32, 32)\n",
    "MXIF_FEATURES = [\"Nucleus PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MAX = np.zeros(len(MXIF_FEATURES))\n",
    "TOTAL_MIN = np.zeros(len(MXIF_FEATURES))\n",
    "\n",
    "for i, feature in enumerate(MXIF_FEATURES):\n",
    "    for core in train_mxIF:\n",
    "        current_max = core.loc[:,feature].max()\n",
    "        current_min = core.loc[:,feature].min()\n",
    "        if current_max > TOTAL_MAX[i]:\n",
    "            TOTAL_MAX[i] = current_max\n",
    "        if current_min < TOTAL_MIN[i]:\n",
    "            TOTAL_MIN[i] = current_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    for i in range(len(train_index)):\n",
    "        X = train_mxIF[i].loc[:,'Cell X Position']\n",
    "        Y = train_mxIF[i].loc[:,'Cell Y Position']\n",
    "\n",
    "        for j,(x,y) in enumerate(zip(X, Y)):\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            if np.isnan(x) or np.isnan(y):\n",
    "                continue\n",
    "            if round(x - CELL_SIZE[0]) < 0 or round(x + CELL_SIZE[0]) >= train_cores[i].shape[1]:\n",
    "                continue\n",
    "            if round(y - CELL_SIZE[1]) < 0 or round(y + CELL_SIZE[1]) >= train_cores[i].shape[0]:\n",
    "                continue\n",
    "\n",
    "            cell_image = train_cores[i][round(y-CELL_SIZE[1]):round(y+CELL_SIZE[1]),\n",
    "                                        round(x-CELL_SIZE[0]):round(x+CELL_SIZE[0])] / 255\n",
    "                \n",
    "            cell_features = np.array(train_mxIF[i].loc[j, MXIF_FEATURES], dtype=np.float32)\n",
    "            cell_features = (cell_features - TOTAL_MIN) / TOTAL_MAX\n",
    "                \n",
    "            if np.sum(np.isnan(cell_features)) != 0:\n",
    "                continue\n",
    "\n",
    "            yield (cell_image, cell_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_coordinate_generator():\n",
    "    for i in range(len(train_index)):\n",
    "        X = train_mxIF[i].loc[:,'Cell X Position']\n",
    "        Y = train_mxIF[i].loc[:,'Cell Y Position']\n",
    "\n",
    "        for j,(x,y) in enumerate(zip(X, Y)):\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            if np.isnan(x) or np.isnan(y):\n",
    "                continue\n",
    "            if round(x - CELL_SIZE[0]) < 0 or round(x + CELL_SIZE[0]) >= train_cores[i].shape[1]:\n",
    "                continue\n",
    "            if round(y - CELL_SIZE[1]) < 0 or round(y + CELL_SIZE[1]) >= train_cores[i].shape[0]:\n",
    "                continue\n",
    "\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 21:08:55.146595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/leonkl/anaconda3/envs/LAB/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-08 21:08:55.146642: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 21:08:55.146682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (icme-gpu1): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 21:08:55.147389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "data = tf.data.Dataset.from_generator(data_generator,\n",
    "                                       output_signature=(tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),\n",
    "                                                          tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32)))\n",
    "data = data.batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = \"logs/autoencoder/baseline/20230307-012705\"\n",
    "loaded_model = tf.saved_model.load(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_latent \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i, elem \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[1;32m      3\u001b[0m     latent_he \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mencoder_conv(elem[\u001b[39m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m     latent_mxIF \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mencoder_fnn(elem[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/LAB/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    786\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    788\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    789\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LAB/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:770\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 770\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    771\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    772\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    773\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    775\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LAB/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3012\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3011\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3012\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3014\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3015\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_latent = []\n",
    "for i, elem in enumerate(data):\n",
    "    latent_he = loaded_model.encoder_conv(elem[0])\n",
    "    latent_mxIF = loaded_model.encoder_fnn(elem[1])\n",
    "    latent_he = latent_he.numpy()\n",
    "    latent_mxIF = latent_mxIF.numpy()\n",
    "    data_latent.append(np.concatenate([latent_he, latent_mxIF], axis=1))\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "latent_vectors = np.concatenate(data_latent, axis=0)\n",
    "latent_vectors = whiten(latent_vectors)\n",
    "centroids, distortion = kmeans(latent_vectors, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.zeros(latent_vectors.shape[0])\n",
    "for i in range(latent_vectors.shape[0]):\n",
    "    vector = latent_vectors[i,:]\n",
    "    vector = vector[:,np.newaxis]\n",
    "    dist = np.linalg.norm(np.repeat(vector, K, axis=1) - centroids.T, axis=0)\n",
    "    clusters[i] = np.argmin(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = ['#0000FF', '#8A2BE2', '#FF4040', '#8A360F', '#98F5FF', '#FF6103', '#7FFF00', '#EEE8CD', '#FFB90F', '#556B2F', '#EE1289']\n",
    "colormap = [ImageColor.getcolor(color, \"RGB\") for color in colormap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16551, 144)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_mxIF[0].loc[:,'Cell X Position']\n",
    "Y = train_mxIF[0].loc[:,'Cell Y Position']\n",
    "src = train_cores[0]\n",
    "cells = cell_coordinate_generator()\n",
    "for i, (x, y) in enumerate(cells):\n",
    "    cv.circle(src, (int(x),int(y)), radius=5, color=colormap[int(clusters[i])], thickness=-1)\n",
    "    if i > 15000:\n",
    "        break\n",
    "    \n",
    "cv.imwrite(\"test.png\", src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
