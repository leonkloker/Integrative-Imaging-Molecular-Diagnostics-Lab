{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "print(\"GPUs: {}\".format(len(tf.config.list_physical_devices(\"GPU\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = [\"H-15\", \"A-15\", \"B-11\", \"A-7\", \"F-9\", \"G-3\", \"G-11\", \"H-7\", \"I-13\"]\n",
    "path_cores = \"TMA_cores_M06_M07_panels/M06/Cores/\"\n",
    "path_mxIF = \"Texts_small_coregistered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cores = [cv.imread(path_cores + index + \".png\") for index in train_index]\n",
    "train_mxIF = [pd.read_csv(path_mxIF + index + \".csv\") for index in train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER = 128\n",
    "BATCH = 32\n",
    "VAL_SPLIT = 0.04\n",
    "CELL_SIZE = (32, 32)\n",
    "MXIF_FEATURES = [\"Nucleus PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Nucleus PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PD1 (PPD520) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm FOXP3 (PPD540) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD20 (PPD620) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm CD3 (PPD650) Std Dev (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Mean (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Max (Normalized Counts, Total Weighting)\",\n",
    "                 \"Cytoplasm PANCK (PPD690) Std Dev (Normalized Counts, Total Weighting)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MAX = np.zeros(len(MXIF_FEATURES))\n",
    "TOTAL_MIN = np.zeros(len(MXIF_FEATURES))\n",
    "\n",
    "for i, feature in enumerate(MXIF_FEATURES):\n",
    "    for core in train_mxIF:\n",
    "        current_max = core.loc[:,feature].max()\n",
    "        current_min = core.loc[:,feature].min()\n",
    "        if current_max > TOTAL_MAX[i]:\n",
    "            TOTAL_MAX[i] = current_max\n",
    "        if current_min < TOTAL_MIN[i]:\n",
    "            TOTAL_MIN[i] = current_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(val=False):\n",
    "    def data_generator():\n",
    "        np.random.seed(4)\n",
    "        for i in range(len(train_index)):\n",
    "            X = train_mxIF[i].loc[:,'Cell X Position']\n",
    "            Y = train_mxIF[i].loc[:,'Cell Y Position']\n",
    "\n",
    "            inx = np.random.uniform(size=X.size) > VAL_SPLIT\n",
    "            if val:\n",
    "                inx = np.invert(inx)\n",
    "\n",
    "            rows = np.arange(X.size)\n",
    "\n",
    "            for j,x,y in zip(rows[inx],X[inx],Y[inx]):\n",
    "                x = float(x)\n",
    "                y = float(y)\n",
    "                if np.isnan(x) or np.isnan(y):\n",
    "                    continue\n",
    "                if round(x - CELL_SIZE[0]) < 0 or round(x + CELL_SIZE[0]) >= train_cores[i].shape[1]:\n",
    "                    continue\n",
    "                if round(y - CELL_SIZE[1]) < 0 or round(y + CELL_SIZE[1]) >= train_cores[i].shape[0]:\n",
    "                    continue\n",
    "\n",
    "                cell_image = train_cores[i][round(y-CELL_SIZE[1]):round(y+CELL_SIZE[1]),\n",
    "                                            round(x-CELL_SIZE[0]):round(x+CELL_SIZE[0])] / 255\n",
    "                \n",
    "                cell_features = np.array(train_mxIF[i].loc[j, MXIF_FEATURES], dtype=np.float32)\n",
    "                cell_features = (cell_features - TOTAL_MIN) / TOTAL_MAX\n",
    "                \n",
    "                if np.sum(np.isnan(cell_features)) != 0:\n",
    "                    continue\n",
    "\n",
    "                yield (cell_image, cell_features), (cell_image, cell_features)\n",
    "                \n",
    "    return data_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(get_generator(),\n",
    "                    output_signature=((tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32)),\n",
    "                    (tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32))))\n",
    "val_ds = tf.data.Dataset.from_generator(get_generator(val=True),\n",
    "                    output_signature=((tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32)),\n",
    "                    (tf.TensorSpec(shape=(2*CELL_SIZE[1],2*CELL_SIZE[0],3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(len(MXIF_FEATURES)), dtype=tf.float32))))\n",
    "train_ds = train_ds.shuffle(BUFFER)\n",
    "train_ds = train_ds.batch(BATCH)\n",
    "val_ds = val_ds.batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_separate = tf.saved_model.load(\"logs/autoencoder/baseline_64dim/ffn_dropout/20230314-220323/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedAutoencoder(tf.keras.models.Model):\n",
    "    def __init__(self, latent_dim=(80,20), dropout=(0,0)):\n",
    "        super(CombinedAutoencoder, self).__init__()\n",
    "        self.autoencoder_separate = autoencoder_separate\n",
    "        self.latent_dim_pre = latent_dim[0]\n",
    "        self.latent_dim = latent_dim[1]\n",
    "        self.dropout_encoder = dropout[0]\n",
    "        self.dropout_decoder_ffn = dropout[1]\n",
    "\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        \n",
    "        self.combined_encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim_pre)),\n",
    "            tf.keras.layers.Dense(60),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_encoder),\n",
    "            tf.keras.layers.Dense(40),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_encoder),\n",
    "            tf.keras.layers.Dense(self.latent_dim, activation='tanh')])\n",
    "\n",
    "        self.decoder_conv = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim)),\n",
    "            tf.keras.layers.Dense(units=self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(units=48, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "\n",
    "            tf.keras.layers.Reshape(target_shape=(1, 1, 64)),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=48, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=3, \n",
    "                                            strides=(2, 2), padding='same', activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=(1, 1), activation='sigmoid', padding='same')])\n",
    "\n",
    "        self.decoder_ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim)),\n",
    "            tf.keras.layers.Dense(units=self.latent_dim, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_decoder_ffn),\n",
    "            tf.keras.layers.Dense(units=self.latent_dim),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_decoder_ffn),\n",
    "            tf.keras.layers.Dense(units=self.latent_dim),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_decoder_ffn),\n",
    "            tf.keras.layers.Dense(units=len(MXIF_FEATURES)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dropout(self.dropout_decoder_ffn),\n",
    "            tf.keras.layers.Dense(units=len(MXIF_FEATURES), activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        he_latent = self.autoencoder_separate.encoder_conv(inputs[0], training=False)\n",
    "        mxIF_latent = self.autoencoder_separate.encoder_fnn(inputs[1], training=False)\n",
    "        h = self.concat([he_latent, mxIF_latent])\n",
    "        z = self.combined_encoder(h)\n",
    "        he = self.decoder_conv(z)\n",
    "        mxIF = self.decoder_ffn(z)\n",
    "        return he, mxIF\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedAutoencoder()\n",
    "for x, y in val_ds.take(1):\n",
    "    model(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanAbsoluteError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/autoencoder/combined/\"\n",
    "log_dir_train = log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir_train, histogram_freq=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               restore_best_weights=True, patience=20,\n",
    "                               verbose=0, mode='min')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(log_dir_train, monitor='val_loss',\n",
    "                                verbose = 0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=200, shuffle=True, verbose=2,\n",
    "          callbacks=[tensorboard, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.saved_model.load(\"logs/autoencoder/baseline_big/20230314-085415/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i, elem in enumerate(val_ds):\n",
    "    if i == 40:\n",
    "        predict = loaded_model(elem[0], training=False)\n",
    "        for j in range(32):\n",
    "            cv.imwrite(\"original{}.png\".format(j), elem[0][0][j].numpy()*255)\n",
    "            cv.imwrite(\"predict{}.png\".format(j), predict[0][j].numpy()*255)\n",
    "        break\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109a8611147ef3c1b5bc156e6b3804a3b3068ea33c15f39cb0fa9486c3c659cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
